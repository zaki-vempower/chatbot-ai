#!/bin/bash

echo "ğŸ¦™ Ollama Setup for Local Llama Models"
echo "======================================"
echo ""

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama is not installed."
    echo ""
    echo "ğŸ”§ Installing Ollama..."
    
    # Detect OS and install accordingly
    if [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS
        echo "ğŸ“± Detected macOS - Installing via curl..."
        curl -fsSL https://ollama.ai/install.sh | sh
    elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
        # Linux
        echo "ğŸ§ Detected Linux - Installing via curl..."
        curl -fsSL https://ollama.ai/install.sh | sh
    else
        echo "âŒ Unsupported OS. Please install Ollama manually from https://ollama.ai"
        exit 1
    fi
    
    echo "âœ… Ollama installed successfully!"
else
    echo "âœ… Ollama is already installed"
fi

echo ""
echo "ğŸš€ Starting Ollama service..."

# Start Ollama service in background
if [[ "$OSTYPE" == "darwin"* ]]; then
    # macOS - start as background service
    ollama serve > /dev/null 2>&1 &
    OLLAMA_PID=$!
    echo "Started Ollama service (PID: $OLLAMA_PID)"
else
    # Linux - start with systemd if available, otherwise background
    if command -v systemctl &> /dev/null; then
        sudo systemctl start ollama
        echo "Started Ollama service via systemctl"
    else
        ollama serve > /dev/null 2>&1 &
        OLLAMA_PID=$!
        echo "Started Ollama service (PID: $OLLAMA_PID)"
    fi
fi

# Wait for service to start
echo "â³ Waiting for Ollama service to start..."
sleep 3

# Check if service is running
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama service is running"
else
    echo "âŒ Failed to start Ollama service"
    exit 1
fi

echo ""
echo "ğŸ“¦ Installing recommended Llama models..."
echo ""

# Install Llama2 (default model)
echo "ğŸ”½ Downloading Llama2 (7B) - this may take a while..."
ollama pull llama2

# Install a smaller model for faster responses
echo "ğŸ”½ Downloading Llama2:7b-chat - optimized for chat..."
ollama pull llama2:7b-chat

# Install Code Llama for coding tasks
echo "ğŸ”½ Downloading Code Llama (7B) for coding assistance..."
ollama pull codellama

echo ""
echo "ğŸ‰ Setup complete!"
echo ""
echo "ğŸ“‹ Available models:"
ollama list

echo ""
echo "âš™ï¸  Configuration:"
echo "   â€¢ Ollama Host: http://localhost:11434"
echo "   â€¢ Default Model: llama2"
echo "   â€¢ Service Status: Running"
echo ""
echo "ğŸ”§ Environment Variables (already configured in .env.local):"
echo "   OLLAMA_HOST=http://localhost:11434"
echo "   OLLAMA_MODEL=llama2"
echo ""
echo "ğŸ’¡ Tips:"
echo "   â€¢ Use 'llama2:7b-chat' for better conversation"
echo "   â€¢ Use 'codellama' for programming help"
echo "   â€¢ Run 'ollama list' to see all installed models"
echo "   â€¢ Run 'ollama ps' to see running models"
echo "   â€¢ Models are stored in ~/.ollama/models"
echo ""
echo "ğŸš€ You can now use the 'Llama (Local)' option in the AI Chatbot!"
echo "   No API keys required - everything runs locally!"
